{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Digits\n",
    "Implementing a simple KNN to classify digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep\n",
    "\n",
    "#### Load Reference Files and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2,114 (total) n_kills (28, 28) crops\n",
    "df_28x28 = pd.read_csv('numbers.csv')\n",
    "\n",
    "# 11,024 (total) n_kills & n_pr (38, 28) crops\n",
    "df_38x28 = pd.read_csv('digits_only_numbers.csv')\n",
    "\n",
    "# 7,717 (each) n_kills & n_pr crops (38, 28) & 150 (total) n_tr crops (38, 28)\n",
    "df_38x28_s = pd.read_csv('labeled_screenshots.csv')\n",
    "n_teams_numbers = df_38x28_s[['n_teams_remaining', 'tr_reference_file']].dropna()\n",
    "n_players_numbers = df_38x28_s[['n_players_remaining', 'pr_reference_file']].dropna()\n",
    "n_kills_numbers = df_38x28_s[['n_kills', 'k_reference_file']].dropna()\n",
    "for numbers_group in [n_teams_numbers, n_players_numbers, n_kills_numbers]:\n",
    "    numbers_group.columns = ['numbers', 'file_path']\n",
    "df_38x28_s = pd.concat([n_teams_numbers, n_players_numbers, n_kills_numbers])\n",
    "\n",
    "# combine into unified dataframe of numbers (labels) & file paths\n",
    "df = pd.concat([df_28x28, df_38x28, df_38x28_s], ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean Up Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_val = 153\n",
    "# max_val = 33\n",
    "\n",
    "max_label_sample = False\n",
    "# max_label_sample = 500\n",
    "\n",
    "standard_nulls = True\n",
    "\n",
    "fix_digits = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix digits\n",
    "if fix_digits:\n",
    "    df.numbers.loc[df.numbers == '00'] = 0\n",
    "    for _ in range(160):\n",
    "        if _ <= 152:\n",
    "            df.numbers.loc[df.numbers == f'{_}'] = _\n",
    "            df.numbers.loc[df.numbers == f'{float(_)}'] = _\n",
    "            # relabel: partially blurry > blurry\n",
    "            for e in [f'b{_}', f'{_}b', f'{_}bb', f'b{float(_)}', f'{float(_)}b' f'{float(_)}bb']:\n",
    "                df.numbers.loc[df.numbers == e] = 'b'\n",
    "                if _ < 10:\n",
    "                    df.numbers.loc[df.numbers == f'b0{_}'] = 'b'\n",
    "            # remove all icon issue numbers\n",
    "            for e in [f'i{int(_)}', f'{int(_)}i', f'i{float(_)}', f'{float(_)}i',\n",
    "                      f'i{float(_)}b', f'b{float(_)}i', f'b{int(_)}i', f'i{int(_)}b', f'ie{int(_)}', f'ie{float(_)}',\n",
    "                      f'i{int(_)}e', f'i{float(_)}e']:\n",
    "                df = df.loc[df.numbers != e]\n",
    "            # remove other error issue numbers\n",
    "            for e in [f'e{_}', f'{_}e', f'e{float(_)}', f'{float(_)}e']:\n",
    "                df = df.loc[df.numbers != e]\n",
    "        else:\n",
    "            # remove any numbers over 152\n",
    "            for e in [f'{int(_)}', f'i{int(_)}', f'{int(_)}i', f'i{float(_)}', f'{float(_)}i', \n",
    "                      f'b{int(_)}', f'{int(_)}b',\n",
    "                      f'e{int(_)}', f'e{float(_)}']:\n",
    "                df = df.loc[df.numbers != e]\n",
    "\n",
    "# fix nulls (standardize)\n",
    "if standard_nulls:\n",
    "    df.numbers.loc[df.numbers == 'b'] = ''\n",
    "    df.numbers.loc[df.numbers == 'e'] = ''\n",
    "    df.numbers.loc[df.numbers == 'r'] = ''\n",
    "    df.numbers.loc[df.numbers == 'n'] = ''\n",
    "    df.numbers.loc[df.numbers == 'bb'] = ''\n",
    "    df.numbers.loc[df.numbers == 'ib'] = ''\n",
    "    df.numbers.loc[df.numbers == 'ibb'] = ''\n",
    "    df.numbers.loc[df.numbers == 'ie'] = ''\n",
    "    df.numbers.loc[df.numbers == 'nn'] = ''\n",
    "    df.numbers.loc[df.numbers == ''] = 153\n",
    "\n",
    "# 0-9 only\n",
    "if max_val:\n",
    "    df = df.loc[df.numbers != '']\n",
    "    df = df.loc[df.numbers <= max_val]\n",
    "    \n",
    "# limit number of each label\n",
    "if max_label_sample:\n",
    "    for value in df.numbers.unique():\n",
    "        c = len(df.loc[df.numbers==value])\n",
    "        if c > max_label_sample:\n",
    "            temp_df = df.loc[df.numbers == value].sample(max_label_sample)\n",
    "            df = df.loc[df.numbers != value]\n",
    "            df = pd.concat([df, temp_df])\n",
    "        print(f'{value} | {len(df.loc[df.numbers==value])}')\n",
    "\n",
    "# convert numbers column to float\n",
    "try:\n",
    "    df.numbers = df.numbers.astype('float')\n",
    "    print(f'df.numbers.dtype == {df.numbers.dtype}')\n",
    "except:\n",
    "    print(f'df.numbers.dtype == {df.numbers.dtype}')\n",
    "\n",
    "# let's see how it looks\n",
    "print(f'len(df) == {len(df)}')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What target values are in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.numbers.unique()), df.numbers.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_numbers = []\n",
    "for un in df.numbers.unique():\n",
    "    try:\n",
    "        actual_numbers.append(int(un))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "len(actual_numbers)#, sorted(actual_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.numbers.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in Data (Images)\n",
    "For `X`: Make list of lists, each holding an array (image) and its file path. `.flatten()` the arrays so they're 1D.\n",
    "\n",
    "For `y`: Target values are found in the `numbers columns`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X = [[cv.imread(fp).flatten(), fp] if Image.open(fp).size==(38, 28) else [np.array(Image.open(fp).crop((0-3, 0, 28+7, 28))).flatten(), fp] for fp in df.file_path.values]\n",
    "y = df.numbers.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train / Test Split\n",
    "After train/test splitting, split the file paths from the arrays (images) so we have an array of file paths and an array of arrays (images) for training and for testing (4 arrays total).\n",
    "\n",
    "The arrays of file paths (`train_file_paths`, `test_file_paths`) are of no use to our model, and are only recorded so that we can examine particular instances (e.g. to see an incorrectly predicted image)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# keep file paths \n",
    "train_file_paths = np.array([fp for img, fp in X_train])\n",
    "test_file_paths = np.array([fp for img, fp in X_test])\n",
    "\n",
    "X_train = np.array([img for img, fp in X_train])\n",
    "X_test = np.array([img for img, fp in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train#, pd.DataFrame(y_train).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create & Train Model\n",
    "And output an array of predictions (just to see what they look like)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=1, n_jobs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "preds = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_correct = np.sum(preds==y_test)\n",
    "n_possible = len(y_test)\n",
    "\n",
    "print(f'n_correct:  {n_correct}\\nn_possible: {n_possible}\\n% correct:  {n_correct/n_possible*100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Current \n",
    "- 4796 rows, k=1, test_size=0.3, % correct: 93.12022237665045%\n",
    "\n",
    "#### Previous Scores\n",
    "- Week of 31 August 2020\n",
    "    - 1740 rows, k=1, test_size=0.2, % correct: 89.65517241379311%\n",
    "    - 2114 rows, k=1, test_size=0.2, % correct: 88.88888888888889%\n",
    "    - 2114 rows, k=2, test_size=0.2, % correct: 88.65248226950354%\n",
    "    - 2114 rows, k=3, test_size=100, % correct: 94.0% (one off, more range variation than above, higher highs, lower lows)\n",
    "    - 2114 rows, k=1, test_size=100, % correct: 91.0% (consistent, some variation ranging 84-93%)\n",
    "    - 2114 rows, k=1, test_size=0.3, % correct (7 runs avg): 88.008998875%\n",
    "\n",
    "#### Goal Score (18 September 2020)\n",
    "- n rows, k=k, test_size=test_size, % correct: > 94%\n",
    "\n",
    "#### Goal Score (30 September 2020)\n",
    "- n rows, k=k, test_size=test_size, % correct: > 98.1%\n",
    "\n",
    "#### Goal Deployed Score (31 October 2020)\n",
    "- n rows, k=k, test_size=live_feed, % correct: > 95.1%+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's wrong? Predicted v Actual\n",
    "Incorrect predictions on the left, actual values (labels) on the right. (Assumes labels are correct.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_df = pd.DataFrame()\n",
    "\n",
    "comp_df['predicted'] = preds\n",
    "comp_df['actual'] = y_test\n",
    "comp_df['reference_file'] = test_file_paths\n",
    "\n",
    "comp_df.loc[comp_df.predicted != comp_df.actual]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the top 7 targets we are missing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_df.loc[comp_df.predicted != comp_df.actual].actual.value_counts()[:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the top 7 targets we are hitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_df.loc[comp_df.predicted == comp_df.actual].actual.value_counts()[:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each number, print\n",
    "- number\n",
    "- n incorrect predictions\n",
    "- n correct predictions\n",
    "\n",
    "Note: this displays numbers from most incorrectly predicted (total) to least incorrectly predicted, not necessairly the same as least accurate to most accurate (i.e. less representation means less opportunities for error).\n",
    "\n",
    "**Goal**: > 95% accuracy on each possible target (outcome)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_numbers_possible_correct = list(comp_df.loc[comp_df.predicted == comp_df.actual].actual.value_counts())\n",
    "numbers_possible_correct = comp_df.loc[comp_df.predicted == comp_df.actual].actual.value_counts().index\n",
    "\n",
    "n_numbers_possible_incorrect = list(comp_df.loc[comp_df.predicted != comp_df.actual].actual.value_counts())\n",
    "numbers_possible_incorrect = comp_df.loc[comp_df.predicted != comp_df.actual].actual.value_counts().index\n",
    "\n",
    "correct_dct = {}\n",
    "for i in range(len(numbers_possible_correct)):\n",
    "    correct_dct.update({numbers_possible_correct[i]:n_numbers_possible_correct[i]})\n",
    "\n",
    "incorrect_dct = {}\n",
    "for i in range(len(numbers_possible_incorrect)):\n",
    "    incorrect_dct.update({numbers_possible_incorrect[i]:n_numbers_possible_incorrect[i]})\n",
    "    \n",
    "seen = []\n",
    "correct_keys = [k for k in correct_dct.keys()]\n",
    "incorrect_keys = [k for k in incorrect_dct.keys()]\n",
    "\n",
    "# accepted_error = 0.5 * 100\n",
    "accepted_error = 0.4 * 100  # 12 Sept (0.35)\n",
    "# accepted_error = 0.2 * 100  # 19 Sept\n",
    "# accepted_error = 0.08 * 100  # 26 Sept\n",
    "# accepted_error = 0.04999995 * 100  # 3 Oct\n",
    "need_more = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in incorrect_keys:\n",
    "    if i not in seen:\n",
    "        print(i)\n",
    "        print(f'incorrect: {incorrect_dct[i]}')\n",
    "        try:\n",
    "            print(f'# correct: {correct_dct[i]}')\n",
    "            error_per = float(str(incorrect_dct[i]/(incorrect_dct[i]+correct_dct[i])*100)[:7])\n",
    "            print(f'per error: {error_per}%')\n",
    "            if error_per > accepted_error:\n",
    "                try:\n",
    "                    need_more.append(int(i))\n",
    "                except:\n",
    "                    need_more.append(i)\n",
    "        except:\n",
    "            try:\n",
    "                need_more.append(int(i))\n",
    "            except:\n",
    "                need_more.append(i)\n",
    "        print()\n",
    "        seen.append(i)\n",
    "\n",
    "for i in correct_keys:\n",
    "    if i not in seen:\n",
    "        print(i)\n",
    "        print(f'correct: {correct_dct[i]}')\n",
    "        try:\n",
    "            print(f'incorrect: {incorrect_dct[i]}')\n",
    "            error_per = float(str(incorrect_dct[i]/(incorrect_dct[i]+correct_dct[i])*100)[:7])\n",
    "            print(f'per error: {error_per}%')\n",
    "            if error_per > accepted_error:\n",
    "                try:\n",
    "                    need_more.append(int(i))\n",
    "                except:\n",
    "                    need_more.append(i)\n",
    "        except:\n",
    "            pass\n",
    "        print()\n",
    "        seen.append(i)\n",
    "\n",
    "need_more_digits = []\n",
    "need_more_others = []\n",
    "for target in need_more:\n",
    "    try:\n",
    "        need_more_digits.append(int(target))\n",
    "    except:\n",
    "        need_more_others.append(target)\n",
    "        \n",
    "print(f'len(need_more_digits) == {len(need_more_digits)}')\n",
    "print(f'len(need_more_others) == {len(need_more_others)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(need_more_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(need_more_others)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
